PARALLELIZED PYROSETTA FOR MOTIF-TARGET BINDING EVALUATION
===========================================================

GOAL
----
For each candidate motif, compute:
  delta_E = E(complex_AB) - E(target_A) - E(motif_B)

Key energy terms to extract:
  - delta_hbond: sum of hbond_sc, hbond_bb_sc, hbond_sr_bb, hbond_lr_bb
  - delta_fa_rep: change in repulsive VDW (clash indicator)
  - delta_total: total REU change (binding energy proxy)


BASIC SCORING PATTERN
---------------------
```python
import pyrosetta
from pyrosetta.rosetta.core.scoring import ScoreType

pyrosetta.init("-ex1 -ex2 -ignore_unrecognized_res")
sfxn = pyrosetta.get_fa_scorefxn()

def get_energies(pose):
    sfxn(pose)
    energies = pose.energies()
    return {
        'total': pose.energies().total_energy(),
        'fa_rep': energies.total_energies()[ScoreType.fa_rep],
        'hbond': sum([
            energies.total_energies()[ScoreType.hbond_sc],
            energies.total_energies()[ScoreType.hbond_bb_sc],
            energies.total_energies()[ScoreType.hbond_sr_bb],
            energies.total_energies()[ScoreType.hbond_lr_bb],
        ])
    }

# Score complex, then split and score components
complex_E = get_energies(complex_pose)
target_E = get_energies(target_pose)
motif_E = get_energies(motif_pose)

delta = {k: complex_E[k] - target_E[k] - motif_E[k] for k in complex_E}
```


PARALLELIZATION WITH MULTIPROCESSING
------------------------------------
PyRosetta is not thread-safe. Use separate processes, each with its own
PyRosetta initialization.

```python
from multiprocessing import Pool

def evaluate_candidate(pdb_path):
    import pyrosetta
    pyrosetta.init("-ex1 -ex2 -ignore_unrecognized_res", set_logging_handler=None)
    # ... load, score, return dict
    return result

if __name__ == "__main__":
    with Pool(processes=16) as pool:
        results = pool.map(evaluate_candidate, candidate_list)
```

Key points:
- Import pyrosetta inside the worker function
- Use set_logging_handler=None to suppress duplicate logs
- if __name__ == "__main__" guard is required on some platforms


CLUSTER DEPLOYMENT (SLURM EXAMPLE)
----------------------------------
For large candidate sets, use array jobs rather than multiprocessing:

```bash
#!/bin/bash
#SBATCH --array=1-1000
#SBATCH --cpus-per-task=1
#SBATCH --mem=4G
#SBATCH --time=0:30:00

python score_one_candidate.py --index $SLURM_ARRAY_TASK_ID
```

This scales better and handles failures gracefully (rerun failed indices).


PRACTICAL CONSIDERATIONS
------------------------
1. Pre-compute target energy once (it's constant across all candidates)

2. For splitting complex into components, either:
   - Load motif separately from its original PDB
   - Use pose.split_by_chain() if chains are distinct
   - Clone and delete residues if needed

3. Relax before scoring if conformations are approximate:
   - FastRelax with coordinate constraints preserves placement
   - Increases runtime ~100x but gives more meaningful energies

4. Negative delta_hbond and delta_total indicate favorable binding
   High delta_fa_rep (>10 REU) indicates steric clashes

5. For ~1000 candidates with simple scoring: expect ~1-2 hours on 16 cores
   With relaxation: expect ~10-20 hours on 16 cores
